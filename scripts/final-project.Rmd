---
title: "DS421 semester 2 final project"
author: "@xrwang"
date: "5/3/2018"
output: 
  pdf_document:
    latex_engine: xelatex
---

#Contents

This is a final project PDF document for DS421 stitched together from other experiments in this rpo. 

Some major goals were:
- Get satellite data/imagery for village and county names.
- Poke around household income data
- Poke around land use change for a few Taobao villages. 

## Section A
First we'll take a look at household income data from CHIP, an{rd geocode the counties based off a csv of "official administrative codes".

We'll also poke at the data a bit, looking at changes over time.

## Section B


##CHIP

CHIP (China Household Income Project) is put out by the CIID Beijing as a longitudinal survey. It's been happening since 1988 and includes all kinds of juicy stuff including land use.

Load up necessary libraries. Some data is in `.dta` which is Stata file. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)

```

```{r}
library(tidyr)
library(tidyverse)
library(dplyr)
library(foreign)
library(reticulate)
library(haven)
library(ggmap)
library(sf)
library(raster)
library(spData)        # load geographic data
library(httr)
library(jsonlite)
```


```{r}
chips_rur_1988 <- read_dta('data/1988/09836-0002-Data.dta')
chips_rur_1995 <- read_tsv('data/1995/DS0002/03012-0002-Data.tsv')
chips_rur_2002<- read_tsv('data/2002/DS0006/21741-0006-Data.tsv')
chips_rur_2007abc <- read_dta('data/2007 (2008)/RHS_w1_abc.dta')
chips_rur_2007d <- read_dta('data/2007 (2008)/RHS_w1_d.dta')
chips_rur_2007e1 <- read_dta('data/2007 (2008)/RHS_w1_e1.dta')
chips_rur_2007e2 <- read_dta('data/2007 (2008)/RHS_w1_e2.dta')
chips_rur_2007e3 <- read_dta('data/2007 (2008)/RHS_w1_e3.dta')
chips_rur_2007e4 <- read_dta('data/2007 (2008)/RHS_w1_e4.dta')
chips_rur_2007hhiexp <- read_dta('data/2007 (2008)/CHIP2007_income_and_expenditure_20150408.dta')
chips_rur_2008abc <- read_dta('data/2008 (2009)/RHS_w2_abc.dta')
chips_rur_2008d <- read_dta('data/2008 (2009)/RHS_w2_d.dta')
chips_rur_2008e <- read_dta('data/2008 (2009)/RHS_w2_e.dta')
chips_rur_2008f <- read_dta('data/2008 (2009)/RHS_w2_f.dta')
chips_rur_2008hgsg <- read_dta('data/2008 (2009)/RHS_w2_hgsg.dta')
chips_rur_2008hijk <- read_dta('data/2008 (2009)/RHS_w2_hijk.dta')
chips_rur_2008vill <- read_dta('data/2008 (2009)/RHS_w2_vill.dta')
chips_rur_2013 <- read_dta('data/2013/CHIP2013_rural_household_f_income_asset.dta')


name_vill_id_2007 <- read_dta('data/2007 (2008)/name_id_and_village_id_20151010.dta')


```

```{r, echo=FALSE}
#Helper function for dta files


labelDataset <- function(data) {
  correctLabel <- function(x) {

    if(!is.null(attributes(x)$labels)) {
      class(attributes(x)$labels) <- typeof(x)
    }
    return(x)
  }

  for(i in colnames(data)) {
    data[, i] <- correctLabel(data[, i])
  }
  return(data)
}


# Helper function 2

turn_to_csv <- function (data, year) {
  if (year == 1988) {
    
  }
  
  
  
  
}


```


### Table of columns used:

| Year | Net household income | Land cultivated | Number of rooms in House | Fixed production assets | Total household exp on production
|---|---|---|---|---|---|
|1988| HNET88 | LAT | HHO | VHPFP | EFP88 |
|1995| B602 | B801 | B1001 | B804_1 | B7130 |
|2002| na| na| na| na| na|
|2007| income_net | na | na | na | na|
|2009| na | H01 | na | K01 | na|
|2013| F01_1 | L01_1 | na | F07_1 + F07_2 | F02_1 | 


```{r}
# Filter out some data from 1988 because there's missing values. They got rid of missing values in later datasets.

chips_rur_1988_filt <- chips_rur_1988 %>% filter(HNET88 != 99999999, LAT != 999.9, HHO != 99, VHPFP != 99999, EFP88 != 99999999 ) 

base::mean(chips_rur_1988_filt$HNET88)
base::mean(chips_rur_1995$B602)
base::mean(chips_rur_2007hhiexp$income_net)
base::mean(chips_rur_2013$f01_1, na.rm=TRUE)

meanNetIncome <- new_tibble(list(year = c(1988,1995,2007,2013), 
                     meanInNet = c(base::mean(chips_rur_1988_filt$HNET88), base::mean(chips_rur_1995$B602), base::mean(chips_rur_2007hhiexp$income_net), base::mean(chips_rur_2013$f01_1, na.rm=TRUE))))

ggplot(meanNetIncome, aes(year, meanInNet)) +
  geom_line() +
  geom_point() +
  geom_label(label=meanNetIncome$meanInNet, nudge_x = 2, nudge_y = 1)
 
```

```{r}

chips_rur_1988_filt <- chips_rur_1988_filt %>%
  mutate(., PROVCOUNTY = paste0(PROVINCE, COUNTY)) 

chips_rur_1988_filt$UCODE <- substr(chips_rur_1988_filt$UCODE, 1,6)


#A1 is county and city code
chips_rur_1995 %>%
  mutate(., PROVCOUNTY = paste0(PROVINCE, COUNTY))


chips_rur_2007hhiexp$name_id <- substr(chips_rur_2007hhiexp$name_id, 1, 6)

```


##County level variations in income

```{r}
chips_rur_1988_filt %>%
  group_by(UCODE) %>%
  summarise(mean = base::mean(HNET88)) %>%
  mutate(., diff_from_country_mean = mean-base::mean(chips_rur_1988_filt$HNET88))
base::mean(chips_rur_1988_filt$HNET88)

chips_rur_1995 %>%
  group_by(A1) %>%
  summarise(mean=base::mean(B602)) %>%
  mutate(., diff_from_country_mean = mean-base::mean(chips_rur_1995$B602))

chips_rur_2007hhiexp %>%
  group_by(name_id) %>%
  summarise(mean=base::mean(income_net)) %>%
  mutate(., diff_from_country_mean = mean-base::mean(chips_rur_2007hhiexp$income_net))

chips_rur_2013 %>%
  group_by(coun) %>%
  summarise(mean=base::mean(f01_1)) %>%
  mutate(., diff_from_country_mean = mean-base::mean(chips_rur_2013$f01_1, na.rm=TRUE))

```
##dice off last 3 digits from `name id` of chips_rur_2007hhiexp 


```{r}


outlier_values_1988 <- boxplot.stats(chips_rur_1988_filt$HNET88)$out  # outlier values.
outlier_values_1995 <- boxplot.stats(chips_rur_1995$B602)$out  # outlier values.
outlier_values_2007 <- boxplot.stats(chips_rur_2007hhiexp$income_net)$out  # outlier values.
outlier_values_2013 <- boxplot.stats(chips_rur_2013$f01_1)$out  # outlier values.
boxplot(chips_rur_1988_filt$LAT, chips_rur_1988_filt$HNET88)

```


## Geocode county addresses based off the county codes
Data from https://raw.githubusercontent.com/modood/Administrative-divisions-of-China/, basically there's a county code and name for each county. They are not geocoded however (by geocoded I mean "center" of county).


#Add long lat columns to the county code table

```{r}

counties_main <- read_csv('https://raw.githubusercontent.com/modood/Administrative-divisions-of-China/master/dist/areas.csv')

counties_main

counties_main$lat <- 'NA'
counties_main$long <- 'NA'

city_main <- read_csv('https://raw.githubusercontent.com/modood/Administrative-divisions-of-China/master/dist/cities.csv')
province_main <- read_csv('https://raw.githubusercontent.com/modood/Administrative-divisions-of-China/master/dist/provinces.csv')

prov_county_main <- left_join(counties_main, province_main,
                              by=c('provinceCode'='code'))

prov_county_main <- prov_county_main %>%
    rename('cityName' = name.y, 
           'countyName' = name.x) %>%
    mutate(., geocodeAdd = paste0(cityName,countyName))


prov_county_main

```


# Set an API key for ggmap so we don't go over the limit 

```{r, eval=FALSE}

ggmap_credentials()
register_google(key='insertyourkeyhere')
library(ggmap)
ggmap_credentials()

```

## Divide the table into two so as to preempt going over the free geocoding limit
```{r, eval=FALSE}

# pcoun_codes_1 <- filter(prov_county_main, code < 400000)
# pcoun_codes_2 <- filter(prov_county_main, code >= 400000)

```


## Batch geocode using ggmap, h/t shayne lynn
```{r, eval=FALSE}


# infile <- 'pcoun_2'
# data <-pcoun_codes_2
# 
# addresses = data$geocodeAdd
# 
# #define a function that will process googles server responses for us.
# getGeoDetails <- function(address){
#    #use the gecode function to query google servers
#    geo_reply = geocode(address, output='all', messaging=TRUE, override_limit=TRUE)
#    #now extract the bits that we need from the returned list
# 
#    answer <- data.frame(lat=NA, long=NA, accuracy=NA, formatted_address=NA, address_type=NA, status=NA)
#    answer$status <- geo_reply$status
# 
#    #if we are over the query limit - want to pause for an hour
#    while(geo_reply$status == "OVER_QUERY_LIMIT"){
#        print("OVER QUERY LIMIT - Pausing for 1 hour at:")
#        time <- Sys.time()
#        print(as.character(time))
#        Sys.sleep(60*60)
#        geo_reply = geocode(address, output='all', messaging=TRUE, override_limit=TRUE)
#        answer$status <- geo_reply$status
#    }
# 
#    #return Na's if we didn't get a match:
#    if (geo_reply$status != "OK"){
#        return(answer)
#    }
#    #else, extract what we need from the Google server reply into a dataframe:
#    answer$lat <- geo_reply$results[[1]]$geometry$location$lat
#    answer$long <- geo_reply$results[[1]]$geometry$location$lng
#    if (length(geo_reply$results[[1]]$types) > 0){
#        answer$accuracy <- geo_reply$results[[1]]$types[[1]]
#    }
#    answer$address_type <- paste(geo_reply$results[[1]]$types, collapse=',')
#    answer$formatted_address <- geo_reply$results[[1]]$formatted_address
# 
#    return(answer)
# }
# 
# #initialise a dataframe to hold the results
# geocoded <- data.frame()
# # find out where to start in the address list (if the script was interrupted before):
# startindex <- 1
# #if a temp file exists - load it up and count the rows!
# tempfilename <- paste0(infile, '_temp_geocoded.rds')
# if (file.exists(tempfilename)){
#        print("Found temp file - resuming from index:")
#        geocoded <- readRDS(tempfilename)
#        startindex <- nrow(geocoded)+1
#        print(startindex)
# }
# 
# # Start the geocoding process - address by address. geocode() function takes care of query speed limit.
# for (ii in seq(startindex, length(addresses))) {
#    print(paste("Working on index", ii, "of", length(addresses)))
#    #query the google geocoder - this will pause here if we are over the limit.
# 
#    result = getGeoDetails(addresses[ii])
#    print(result$status)
#    result$index <- ii
#    #append the answer to the results file.
#    geocoded <- rbind(geocoded, result)
#    #save temporary results as we are going along
#    saveRDS(geocoded, tempfilename)
# }

# geocodedTable <- data.frame(matrix(ncol = 3, nrow = 1516))
# #now we add the latitude and longitude to the main data
# 
# geocodedTable$status <- geocoded$status
# geocodedTable$formatted_address <- geocoded$formatted_address
# geocodedTable$index <- geocoded$index
# geocodedTable$lat <- geocoded$lat
# geocodedTable$long <- geocoded$long
# geocodedTable$accuracy <- geocoded$accuracy
# 
# 
# 
# #finally write it all to the output files
# saveRDS(data, paste0("../data/", infile ,"_geocoded.rds"))
# write.table(geocodedTable, file=paste0("", infile ,"_geocoded.csv"), sep=",", row.names=FALSE)

```

# Now there's two tables because we split the county list, put together vertically

```{r}
a <- read_csv('data/pcoun_1_geocoded.csv')
b <- read_csv('data/pcoun_2_geocoded.csv')
geocoded_areas <- bind_rows(a,b)

prov_county_main$lat <- geocoded_areas$lat
prov_county_main$long <- geocoded_areas$long

prov_county_main$engAdd <- geocoded_areas$formatted_address

#One big geocoded table
prov_county_main
```

## Income maps
```{r}

#Join CHIP data to geocoded tabel, make a map






```

# Taobao villages 

## Geocoding taobao villages

There are 1312 taobao villages as of 2017. 

I have data for Taobao villages from 2014-2016


Testing out the geocoding response, put the province and village together
(省 column + 村 column, separate by comma )


```{r}
taobao_villages_2016 <- read_csv('data/taobao-village-list/taobao_village_list_2016.csv',
                            col_names = c(
                              'sheng',
                              'shi',
                              'xian',
                              'xiangzhen',
                              'cun'
                            ),
                            skip=1)

taobao_villages_2015 <- readxl::read_excel('data/taobao-village-list/taobao-village-list-2015.xlsx',
                            col_names = c(
                              'sheng',
                              'shi',
                              'xian',
                              'xiangzhen',
                              'cun'
                            ),
                            skip=1)
taobao_villages_2015 <- taobao_villages_2015 %>% 
  mutate(., geocodeAdd = paste0(sheng,cun))

taobao_villages_2014 <- readxl::read_excel('data/taobao-village-list/taobao-village-list-2014.xlsx',
                            col_names = c(
                              'sheng',
                              'shi',
                              'xian',
                              'xiangzhen',
                              'cun',
                              'goods_made'
                            ),
                            skip=1)

```

```{r}
taobao_villages_2016 <- taobao_villages_2016 %>% mutate(., geocodeAdd = paste0(sheng,cun))
taobao_villages_2016 %>% count(xiangzhen)


taobao_villages_subset_2016 <- subset(taobao_villages_2016, xiangzhen == '三合镇' )
#Go through this list and geocode them all !

taobao_villages_subset_2016
token <- Sys.getenv("GAPI1")
#https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA&key=YOUR_API_KEY
geocode_the_place_f <- function(geocodeAdd,...) {

  #http://api.iucnredlist.org/index/species/Acaena-exigua.json
    json <- httr::GET(paste0('https://maps.googleapis.com/maps/api/geocode/json?address=', geocodeAdd, '&key=', token))
  
    if (status_code(json) == 200) {
      results <- fromJSON(content(json, as='text'))$results$geometry$location

    } else {
    NULL
    }
}

taobao_villages_2016_geocoded <- taobao_villages_subset_2016 %>%
  bind_cols(., pmap_df(., geocode_the_place_f))
  

taobao_villages_2016_geocoded

```
